{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from functools import reduce\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "    \n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "from keras.datasets.data_utils import get_file\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Merge\n",
    "from keras.layers import recurrent\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "'''\n",
    "Trains two recurrent neural networks based upon a story and a question.\n",
    "The resulting merged vector is then queried to answer a range of bAbI tasks.\n",
    "\n",
    "For the resources related to the bAbI project, refer to:\n",
    "https://research.facebook.com/researchers/1543934539189348\n",
    "\n",
    "Code forked from https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple', '?']\n",
    "    '''\n",
    "    return [x.strip() for x in re.split('(\\W+)?', sent) if x.strip()]\n",
    "\n",
    "\n",
    "def parse_stories(lines, only_supporting=False):\n",
    "    '''Parse stories provided in the bAbi tasks format\n",
    "\n",
    "    If only_supporting is true, only the sentences that support the answer are kept.\n",
    "    '''\n",
    "    data = []\n",
    "    story = []\n",
    "    for line in lines:\n",
    "        line = line.decode('utf-8').strip()\n",
    "        nid, line = line.split(' ', 1)\n",
    "        nid = int(nid)\n",
    "        if nid == 1:\n",
    "            story = []\n",
    "        if '\\t' in line:\n",
    "            q, a, supporting = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            substory = None\n",
    "            if only_supporting:\n",
    "                # Only select the related substory\n",
    "                supporting = map(int, supporting.split())\n",
    "                substory = [story[i - 1] for i in supporting]\n",
    "            else:\n",
    "                # Provide all the substories\n",
    "                substory = [x for x in story if x]\n",
    "            data.append((substory, q, a))\n",
    "            story.append('')\n",
    "        else:\n",
    "            sent = tokenize(line)\n",
    "            story.append(sent)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_stories(f, only_supporting=False, max_length=None):\n",
    "    '''Given a file name, read the file, retrieve the stories, and then convert the sentences into a single story.\n",
    "\n",
    "    If max_length is supplied, any stories longer than max_length tokens will be discarded.\n",
    "    '''\n",
    "    data = parse_stories(f.readlines(), only_supporting=only_supporting)\n",
    "    flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "    data = [(flatten(story), q, answer) for story, q, answer in data if not max_length or len(flatten(story)) < max_length]\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_json_file(**kwargs):\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Saves dictionary in path.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dict_to_save = kwargs[\"dict_to_save\"]\n",
    "    path = kwargs[\"path\"]\n",
    "    with open(path,'wb') as fp:\n",
    "        json.dump(dict_to_save, fp)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "class BabiPreditor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def predictors_ai_interface(self, **kwargs):\n",
    "\n",
    "            \"\"\"\n",
    "            This is the method used by Predictors.ai to interact with the model.\n",
    "\n",
    "            Inputs:\n",
    "\n",
    "            - pipe_id (integer): id of the pipe that has to be used.\n",
    "\n",
    "            - input_data (dictionary): dictionary that contains the input data. The keys of the dictionary \n",
    "            correspond to the names of the inputs specified in models_definition.json for the selected pipe.\n",
    "            Each key has an associated value. For the input variables the associated value is the value\n",
    "            of the variable, whereas for the input files the associated value is its filename. \n",
    "\n",
    "            - input_files_dir (string): Relative path of the directory where the input files are stored\n",
    "            (the algorithm has to read the input files from there).\n",
    "            - output_files_dir (string): Relative path of the directory where the output files must be stored\n",
    "            (the algorithm must store the output files in there).\n",
    "\n",
    "            Outputs:\n",
    "\n",
    "            - output_data (dictionary): dictionary that contains the output data. The keys of the dictionary \n",
    "            correspond to the names of the outputs specified in models_definition.json for the selected pipe. \n",
    "            Each key has an associated value. For the output variables the associated value is the value\n",
    "            of the variable, whereas for the output files the associated value is its filename.  \n",
    "            \"\"\"\n",
    "\n",
    "            pipe_id = kwargs['pipe_id']\n",
    "            input_data = kwargs['input_data']\n",
    "            input_files_dir = kwargs['input_files_dir']\n",
    "            output_files_dir = kwargs['output_files_dir']\n",
    "\n",
    "            output_data = self.predict(pipe_id, input_data, input_files_dir, output_files_dir)\n",
    "\n",
    "            return output_data\n",
    "        \n",
    "        \n",
    "    def train(self):\n",
    "        \n",
    "        print(\"training...\")\n",
    "    \n",
    "        task_from = 16\n",
    "        task_until = 18\n",
    "        \n",
    "        RNN = recurrent.GRU\n",
    "        EMBED_HIDDEN_SIZE = 50\n",
    "        SENT_HIDDEN_SIZE = 100\n",
    "        QUERY_HIDDEN_SIZE = 100\n",
    "        BATCH_SIZE = 32\n",
    "\n",
    "        babi_dir = \"./tasks_1-20_v1-2/en-10k/\"\n",
    "        train = []\n",
    "        test = []\n",
    "        print(\"Files for training:\")\n",
    "        for filename in sorted(os.listdir(babi_dir))[task_from:task_until]:\n",
    "            print(filename)\n",
    "            filepath = babi_dir + filename\n",
    "            f = open(filepath, 'r')\n",
    "            if \"train\" in filename:\n",
    "                train += get_stories(f)\n",
    "            else:\n",
    "                test += get_stories(f)\n",
    "        print(\"------------\")\n",
    "\n",
    "        vocab = sorted(reduce(lambda x, y: x | y, (set(story + q + [answer]) for story, q, answer in train + test)))\n",
    "        # Reserve 0 for masking via pad_sequences\n",
    "        self.vocab_size = len(vocab) + 1\n",
    "        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "        self.word_idx_inv = dict(zip(self.word_idx.values(),self.word_idx.keys()))\n",
    "        self.story_maxlen = max(map(len, (x for x, _, _ in train + test)))\n",
    "        self.query_maxlen = max(map(len, (x for _, x, _ in train + test)))\n",
    "\n",
    "        X, Xq, Y = self.vectorize_stories_querys_answers(train)\n",
    "\n",
    "        sentrnn = Sequential()\n",
    "        sentrnn.add(Embedding(self.vocab_size, EMBED_HIDDEN_SIZE, mask_zero=True))\n",
    "        sentrnn.add(RNN(SENT_HIDDEN_SIZE, return_sequences=False))\n",
    "\n",
    "        qrnn = Sequential()\n",
    "        qrnn.add(Embedding(self.vocab_size, EMBED_HIDDEN_SIZE))\n",
    "        qrnn.add(RNN(QUERY_HIDDEN_SIZE, return_sequences=False))\n",
    "\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Merge([sentrnn, qrnn], mode='concat'))\n",
    "        self.model.add(Dense(self.vocab_size, activation='softmax'))\n",
    "\n",
    "        self.model.compile(optimizer='adam', loss='categorical_crossentropy', class_mode='categorical')\n",
    "\n",
    "        best_acc = 0\n",
    "        keep_training = True\n",
    "        patience = 5\n",
    "        patience_counter = 0\n",
    "        while keep_training:\n",
    "            history = self.model.fit([X, Xq], Y, batch_size=BATCH_SIZE, nb_epoch=1, validation_split=0.1,\n",
    "                                     show_accuracy=True)\n",
    "            current_acc = history.history['val_acc']\n",
    "            if current_acc > best_acc:\n",
    "                model_weights = self.model.get_weights()\n",
    "                best_acc = current_acc\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter > patience:\n",
    "                    self.model.set_weights(model_weights)\n",
    "                    keep_training = False\n",
    "            print(\"current acc: \" + str(current_acc))\n",
    "            print(\"best acc: \" + str(best_acc))\n",
    "            print(\"------------\")\n",
    "\n",
    "        test_accs = {}\n",
    "        for filename in sorted(os.listdir(babi_dir))[task_from:task_until]:\n",
    "            if \"test\" in filename:\n",
    "                filepath = babi_dir + filename\n",
    "                f = open(filepath, 'r')\n",
    "                test = get_stories(f)\n",
    "                tX, tXq, tY = self.vectorize_stories_querys_answers(test)\n",
    "                loss, acc = self.model.evaluate([tX, tXq], tY, batch_size=BATCH_SIZE, show_accuracy=True)\n",
    "                test_accs[filename] = acc\n",
    "                print(\"test acc: \" + str(acc))\n",
    "        \n",
    "        self.generate_scores_json(test_accs)\n",
    "        self.generate_model_definition()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def vectorize_stories_querys_answers(self, data):\n",
    "        X = []\n",
    "        Xq = []\n",
    "        Y = []\n",
    "        for story, query, answer in data:\n",
    "            x = [self.word_idx[w] for w in story]\n",
    "            xq = [self.word_idx[w] for w in query]\n",
    "            y = np.zeros(self.vocab_size)\n",
    "            y[self.word_idx[answer]] = 1\n",
    "            X.append(x)\n",
    "            Xq.append(xq)\n",
    "            Y.append(y)\n",
    "        return pad_sequences(X, maxlen=self.story_maxlen), pad_sequences(Xq, maxlen=self.query_maxlen), np.array(Y)\n",
    "    \n",
    "   \n",
    "    def vectorize_stories_querys(self, data):\n",
    "        X = []\n",
    "        Xq = []\n",
    "        for story, query in data:\n",
    "            x = [self.word_idx[w] for w in story]\n",
    "            xq = [self.word_idx[w] for w in query]\n",
    "            X.append(x)\n",
    "            Xq.append(xq)\n",
    "        return pad_sequences(X, maxlen=self.story_maxlen), pad_sequences(Xq, maxlen=self.query_maxlen)\n",
    "    \n",
    "    \n",
    "    def generate_scores_json(self, test_accs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculate scores.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        scores = []\n",
    "\n",
    "        score = {}\n",
    "        score['name'] = 'Accuracy'\n",
    "        score['summary_name'] = 'Average accuracy'\n",
    "        score['summary_value'] = sum(test_accs.values())/float(len(test_accs))\n",
    "        score['class_wise'] = {}\n",
    "        score['class_wise']['names'] = list(test_accs.keys())\n",
    "        score['class_wise']['values'] = list(test_accs.values())\n",
    "        scores.append(score)\n",
    "    \n",
    "        scores_out = {}\n",
    "        scores_out[\"scores\"] = scores\n",
    "        scores_out[\"schema_version\"] = \"0.02\"\n",
    "\n",
    "        save_json_file(dict_to_save=scores_out, path=\"./scores.json\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    \n",
    "    def generate_model_definition(self):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns model_definition.json dictionary.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        model_definition = {}\n",
    "        model_definition[\"name\"] = \"Babi predictor\"\n",
    "        model_definition[\"schema_version\"] = \"0.02\"\n",
    "        model_definition[\"environment_name\"] = \"python2.7.9_October25th2015\"\n",
    "        model_definition[\"description\"] = \"<b>This predictor answers questions contained in the bAbI size reasoning task.</b><br />\" \\\n",
    "                                          \"It is based on the sample code available for the Keras library at https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py<br />\" \\\n",
    "                                          \"More info about the bAbI project at https://research.facebook.com/researchers/1543934539189348<br />\" \\\n",
    "                                          'The following words are allowed (case sensitive): \"' + '\", \"'.join(bp.word_idx.keys()) + '\".<br /><br />' \\\n",
    "                                          \"<b>Input story example:</b><br />\" \\\n",
    "                                          \"The box of chocolates fits inside the chest. \" \\\n",
    "                                          \"The container is bigger than the chest. \" \\\n",
    "                                          \"The box of chocolates is bigger than the chocolate. \" \\\n",
    "                                          \"The chest fits inside the container. \" \\\n",
    "                                          \"The suitcase is bigger than the chocolate. \" \\\n",
    "                                          \"<br /><br />\" \\\n",
    "                                          \"<b>Input questions examples:</b><br />\" \\\n",
    "                                          \"Does the chest fit in the chocolate?<br />\" \\\n",
    "                                          \"Is the box of chocolates bigger than the container?<br />\" \\\n",
    "                                          \"Does the chocolate fit in the chest?<br />\" \\\n",
    "                                          \"Is the chocolate bigger than the chest?<br />\" \\\n",
    "                                          \"Does the box of chocolates fit in the container?<br />\"\n",
    "        model_definition[\"retraining_allowed\"] = False\n",
    "        model_definition[\"base_algorithm\"] = \"Neural Network\"     \n",
    "        model_definition[\"score_minimized\"] = \"accuracy\"        \n",
    "\n",
    "        pipes = self.get_pipes()\n",
    "        model_definition[\"pipes\"] = pipes\n",
    "\n",
    "        save_json_file(dict_to_save=model_definition, path=\"./model_definition.json\")\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def get_pipes(self, **kwargs):\n",
    "\n",
    "        \"\"\"\n",
    "        Returns pipes.json dictionary.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        pipes = [ \n",
    "                    {\n",
    "                        \"id\": 0,\n",
    "                        \"action\": \"predict\",\n",
    "                        \"name\":\"One by one prediction\",\n",
    "                        \"description\": \"Please input one story and one question.\",\n",
    "                        \"inputs\": [\n",
    "                            {\n",
    "                                \"name\": \"Story\",\n",
    "                                \"type\": \"variable\",\n",
    "                                \"variable_type\": \"string\", \n",
    "                                \"required\": True\n",
    "                            }, \n",
    "                            {\n",
    "                                \"name\": \"Question\",\n",
    "                                \"type\": \"variable\",\n",
    "                                \"variable_type\": \"string\", \n",
    "                                \"required\": True\n",
    "                            }\n",
    "                        ],\n",
    "                        \"outputs\": [\n",
    "                            {\n",
    "                                \"name\": \"Answer\",\n",
    "                                \"type\": \"variable\",\n",
    "                                \"variable_type\": \"string\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                ]\n",
    "\n",
    "        return pipes\n",
    "\n",
    "    \n",
    "    def predict(self, pipe_id, input_data, input_files_dir, output_files_dir):\n",
    "        \n",
    "        story = input_data[\"Story\"]\n",
    "        query = input_data[\"Question\"]\n",
    "\n",
    "        story = story.replace(\"\\n\", \" \").replace(\".\", \" . \").replace(\"?\", \" ? \").split(\" \")\n",
    "        query = query.replace(\"\\n\", \" \").replace(\".\", \" . \").replace(\"?\", \" ? \").split(\" \")\n",
    "        story = [word for word in story if word != \"\"]\n",
    "        query = [word for word in query if word != \"\"]\n",
    "        \n",
    "        print(story)\n",
    "        print(\"......\")\n",
    "        print(query)\n",
    "        print(\"......\")\n",
    "        \n",
    "        if len(story) > self.story_maxlen:\n",
    "            output = {\"Answer\": \"The story is too long.\"}\n",
    "            \n",
    "        elif len(story) > self.query_maxlen:\n",
    "            output = {\"Answer\": \"The question is too long.\"}\n",
    "            \n",
    "        try:\n",
    "            X, Xq = self.vectorize_stories_querys([(story, query)])\n",
    "            preds = self.model.predict([X, Xq])\n",
    "            answer = self.word_idx_inv[np.argmax(preds[0])]\n",
    "            output = {\"Answer\": answer}\n",
    "        except:\n",
    "            output = {\"Answer\": \"Please use allowed words only.\"}\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bp = BabiPreditor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "Files for training:\n",
      "qa18_size-reasoning_test.txt\n",
      "qa18_size-reasoning_train.txt\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 117s - loss: 0.7715 - acc: 0.4936 - val_loss: 0.7002 - val_acc: 0.5270\n",
      "current acc: [0.52700000000000002]\n",
      "best acc: [0.52700000000000002]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 114s - loss: 0.5278 - acc: 0.6888 - val_loss: 0.1368 - val_acc: 0.9070\n",
      "current acc: [0.90700000000000003]\n",
      "best acc: [0.90700000000000003]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 116s - loss: 0.1346 - acc: 0.9138 - val_loss: 0.1129 - val_acc: 0.9370\n",
      "current acc: [0.93700000000000006]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 114s - loss: 0.1276 - acc: 0.9154 - val_loss: 0.1341 - val_acc: 0.9080\n",
      "current acc: [0.90800000000000003]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 116s - loss: 0.1256 - acc: 0.9192 - val_loss: 0.1226 - val_acc: 0.8990\n",
      "current acc: [0.89900000000000002]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 113s - loss: 0.1245 - acc: 0.9181 - val_loss: 0.1276 - val_acc: 0.9030\n",
      "current acc: [0.90300000000000002]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 113s - loss: 0.1243 - acc: 0.9169 - val_loss: 0.1195 - val_acc: 0.9070\n",
      "current acc: [0.90700000000000003]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 113s - loss: 0.1263 - acc: 0.9170 - val_loss: 0.1299 - val_acc: 0.9040\n",
      "current acc: [0.90400000000000003]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "9000/9000 [==============================] - 113s - loss: 0.1221 - acc: 0.9208 - val_loss: 0.1340 - val_acc: 0.9070\n",
      "current acc: [0.90700000000000003]\n",
      "best acc: [0.93700000000000006]\n",
      "------------\n",
      "1000/1000 [==============================] - 1s     \n",
      "test acc: 0.894\n"
     ]
    }
   ],
   "source": [
    "# create and train predictor\n",
    "bp = BabiPreditor()\n",
    "bp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)\n",
    "\n",
    "# save trained model\n",
    "import cPickle as pickle\n",
    "with open(\"./babi_predictor.pk\",'wb') as fp:\n",
    "    pickle.dump(bp, fp, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = \"The box of chocolates fits inside the chest.\\\n",
    "The container is bigger than the chest.\\\n",
    "The box of chocolates is bigger than the chocolate.\\\n",
    "The chest fits inside the container.\\\n",
    "The suitcase is bigger than the chocolate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"Does the chest fit in the chocolate?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"Is the box of chocolates bigger than the container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"Does the chocolate fit in the chest?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"Is the chocolate bigger than the chest?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q = \"Does the box of chocolates fit in the container?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'box', 'of', 'chocolates', 'fits', 'inside', 'the', 'chest', '.', 'The', 'container', 'is', 'bigger', 'than', 'the', 'chest', '.', 'The', 'box', 'of', 'chocolates', 'is', 'bigger', 'than', 'the', 'chocolate', '.', 'The', 'chest', 'fits', 'inside', 'the', 'container', '.', 'The', 'suitcase', 'is', 'bigger', 'than', 'the', 'chocolate', '.']\n",
      "......\n",
      "['Does', 'the', 'box', 'of', 'chocolates', 'fit', 'in', 'the', 'container', '?']\n",
      "......\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Answer': u'yes'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_id = 0\n",
    "input_files_dir = \"\"\n",
    "output_files_dir = \"\"\n",
    "input_data = {}\n",
    "input_data[\"Story\"] = s\n",
    "input_data[\"Question\"] = q\n",
    "bp.predictors_ai_interface(pipe_id=pipe_id, input_files_dir=input_files_dir, output_files_dir=output_files_dir, input_data=input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
